{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_similarity,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Faithfulness - Measures the factual consistency of the answer to the context based on the question.\n",
    "# Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.\n",
    "# Answer_relevancy - Measures how relevant the answer is to the question.\n",
    "# Context_recall - Measures the retrieverâ€™s ability to retrieve all necessary information required to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
