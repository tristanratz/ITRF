{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from qdrant_client import models, QdrantClient\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from model.llm import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387fc21c5a934f6ca01283596b6865c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = LLM(size=7, quantized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "itrf_dataset_buffer = []\n",
    "k=10\n",
    "options_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the dataset attributes\n",
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = \"retriever\"\n",
    "\n",
    "#tokenizer = LlamaTokenizer.from_pretrained(\"../models/llama7b\", device_map='cuda')\n",
    "embedding = HuggingFaceBgeEmbeddings(model_name=\"../models/retriever/bge-base-en-v1.5\", model_kwargs={\"device\": \"cuda:1\"})\n",
    "\n",
    "# Create the retriever\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "db = Qdrant(client, \n",
    "            collection_name=col_name,\n",
    "            embeddings=embedding,\n",
    "            )\n",
    "\n",
    "seed = 4048\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def search(query: str, k: int = 3):\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            results = db.similarity_search_with_score(query, k=k)\n",
    "            if results:\n",
    "                success = True \n",
    "        except:\n",
    "            print(f\"Error with example {query}, retrying in 0.2s\")\n",
    "            time.sleep(0.2)\n",
    "    return results\n",
    "\n",
    "# This function creates an example with the query and the prediction and the top k results\n",
    "def make_example(query: str, ground_truth:str, dataset_name:str, context = None, example_id = None, k: int = 3, split = \"llm\", retrieval = True, task=\"\", domain=\"\"):\n",
    "    contexts = []\n",
    "    if retrieval:\n",
    "        # Search for the query\n",
    "        results = search(query, k=k)\n",
    "\n",
    "        # Get the softmax of the scores\n",
    "        retriever_softmax = softmax([result[1] for result in results])\n",
    "        context_texts = [llm.format_prompt(query, result[0].page_content) for result in results]\n",
    "        llm_scores = llm.to_tokens_and_logprobs(context_texts, [ground_truth] * k)[1]\n",
    "        llm_softmax = softmax(llm_scores)\n",
    "\n",
    "        # Get the text of the results\n",
    "        contexts = [{\n",
    "            \"text\": result[0].page_content, \n",
    "            \"src\": result[0].metadata[\"src\"] if \"src\" in result[0].metadata.keys() else \"unknown\", \n",
    "            \"id\": result[0].metadata[\"id\"] if \"id\" in result[0].metadata.keys() else result[0].metadata[\"title\"],\n",
    "            \"retriever_score\": result[1],\n",
    "            \"llm_score\": lscore,\n",
    "            \"retriever_softmax\": rsoft, \n",
    "            \"llm_softmax\": lsoft, \n",
    "            \"llm_weighted_softmax\": rsoft * lsoft, \n",
    "            \"original_context\": False } \n",
    "            for result, rsoft, lsoft, lscore in zip(results, retriever_softmax, llm_softmax, llm_scores)]\n",
    "    \n",
    "    # Add the original context\n",
    "    if context:\n",
    "        contexts.append({\n",
    "            \"text\": context, \n",
    "            \"src\": dataset_name, \n",
    "            \"id\": str(example_id), \n",
    "            \"original_context\": True\n",
    "            })\n",
    "\n",
    "    return { \n",
    "        \"split\": split, \n",
    "        \"query\": query, \n",
    "        \"ground_truth\": ground_truth, \n",
    "        \"contexts\": contexts,\n",
    "        \"src\": dataset_name, \n",
    "        \"id\": str(example_id),\n",
    "        \"task\": task,\n",
    "        \"domain\": domain, \n",
    "        }\n",
    "\n",
    "itrf_dataset_buffer = []\n",
    "itrf = DataFrame(columns=[\"split\", \"query\", \"prediction\", \"context\", \"src\", \"id\", \"context_src\", \"context_id\", \"original_context\", \"task\", \"domain\"])\n",
    "\n",
    "def make_examples(query: str, prediction:str, dataset_name:str, context = None, example_id = None, k: int = 3, retrieval = True, task=\"\", domain=\"\"):\n",
    "    examples = []\n",
    "    ex = make_example(query, prediction, dataset_name, context, example_id, k, retrieval=retrieval, task=task, domain=domain)\n",
    "    for c in ex[\"contexts\"]:\n",
    "        examples.append({ \n",
    "            \"split\": ex[\"split\"], \n",
    "            \"query\": ex[\"query\"], \n",
    "            \"ground_truth\": ex[\"ground_truth\"], \n",
    "\n",
    "            \"retriever_score\": c[\"retriever_score\"],\n",
    "            \"llm_score\": c[\"llm_score\"],\n",
    "            \"retriever_softmax\": c[\"retriever_softmax\"], \n",
    "            \"llm_softmax\": c[\"llm_softmax\"], \n",
    "            \"llm_weighted_softmax\": c[\"llm_weighted_softmax\"], \n",
    "\n",
    "            \"context\": c[\"text\"], \n",
    "            \"src\": ex[\"src\"], \n",
    "            \"id\": str(ex[\"id\"]), \n",
    "            \"context_src\": c[\"src\"], \n",
    "            \"context_id\": str(c[\"id\"]), \n",
    "            \"original_context\": c[\"original_context\"],\n",
    "            \"task\": task,\n",
    "            \"domain\": domain,\n",
    "            })\n",
    "    return examples\n",
    "\n",
    "def save_example(i, start, last_time, example, dname, force=False):\n",
    "    save_examples(i, start, last_time, [example], dname, force=force)\n",
    "\n",
    "def save_examples(i, start, last_time, examples, dname, force=False):\n",
    "    global itrf_dataset_buffer\n",
    "    global itrf\n",
    "    # Save the dataset to a file\n",
    "    itrf_dataset_buffer.extend(examples)\n",
    "    \n",
    "    if i % 100 == 0 or force:\n",
    "        current_time = time.time()\n",
    "        print(f\"Processed {i} {dname} examples, time: {str(timedelta(seconds=(last_time - start)))}, last 100 in {str(timedelta(seconds=(current_time - last_time)))}\")\n",
    "        last_time = current_time\n",
    "        if len(itrf_dataset_buffer) > 0:\n",
    "            if itrf.empty:\n",
    "                itrf = DataFrame(itrf_dataset_buffer)\n",
    "            else:\n",
    "                df = DataFrame(itrf_dataset_buffer)\n",
    "                itrf = pd.concat([itrf, df])\n",
    "            itrf_dataset_buffer.clear()\n",
    "        itrf.to_parquet(\"../data/dataset/itrf_dataset_reranker1.parquet\")\n",
    "        itrf.to_csv(\"../data/dataset/itrf_dataset_reranker1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open domain questioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_openqa = [\"tau/commonsense_qa\", \"math_qa\", \"web_questions\", \"wiki_qa\", \"yahoo_answers_qa\", \"freebase_qa\", \"ms_marco\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tau/commonsense_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_examples(query: str, prediction:str, dataset_name:str, context = None, example_id = None, k: int = 3, retrieval = True)\n",
    "dname = datasets_openqa[0]\n",
    "dataset = load_dataset(dname, split=\"train\")\n",
    "shuffled = iter(dataset.shuffle(seed=seed))\n",
    "start = time.time()\n",
    "last_time = start\n",
    "for i in range(n):\n",
    "    example = next(shuffled)\n",
    "\n",
    "    options = \"\"\n",
    "    if options_enabled:\n",
    "        options = \"\\n Options: \"\n",
    "    answer = -1\n",
    "    for idx, o in enumerate(example[\"choices\"][\"label\"]):\n",
    "        if options_enabled:\n",
    "            options += f\"{o}: {example['choices']['text'][idx]}, \"\n",
    "        if example[\"answerKey\"] == o:\n",
    "            answer = idx\n",
    "    if options_enabled: \n",
    "        options += \"\\n\"\n",
    "\n",
    "    query = example[\"question\"] + options\n",
    "    if options_enabled:\n",
    "        prediction = f\"{example['answerKey']}) {example['choices']['text'][answer]}\"\n",
    "    else:\n",
    "        prediction =  f\"{example['choices']['text'][answer]}\"\n",
    "    example_id = example[\"id\"] + \"_\" + example[\"question_concept\"]\n",
    "    examples = make_examples(query, prediction, dname, example_id=example_id, k=k, retrieval=True, task=\"mc\", domain=\"openqa\")\n",
    "    \n",
    "    save_examples(i, start, last_time, examples, dname)\n",
    "\n",
    "itrf_dataset_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itrf_dataset_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FreebaseQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FreebaseQA-train-234'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dname = datasets_openqa[5]\n",
    "dataset = load_dataset(dname, split=\"train\")\n",
    "\n",
    "dataset[234][\"Question-ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = datasets_openqa[5]\n",
    "dataset = load_dataset(dname, split=\"train\")\n",
    "shuffled = iter(dataset.shuffle(seed=2024))\n",
    "start = time.time()\n",
    "last_time = start\n",
    "for i in range(n):\n",
    "    example = next(shuffled)\n",
    "\n",
    "    query = example[\"RawQuestion\"]\n",
    "    prediction =  example['Parses'][\"Answers\"][0][\"AnswersName\"][0][0]\n",
    "    example_id = example[\"Question-ID\"]\n",
    "    examples = make_examples(query, prediction, dname, example_id=example_id, k=k, retrieval=True, task=\"qa\", domain=\"openqa\")\n",
    "    \n",
    "    save_examples(i, start, last_time, examples, dname)\n",
    "\n",
    "itrf_dataset_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MS Marco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dname = datasets_openqa[6]\n",
    "dataset = load_dataset(dname, 'v2.1', split=\"train\")\n",
    "shuffled = iter(dataset.shuffle(seed=4048))\n",
    "start = time.time()\n",
    "empty_samples = []\n",
    "last_time = start\n",
    "for i in range(3500):\n",
    "    example = next(shuffled)\n",
    "\n",
    "    query = example[\"query\"]\n",
    "    example_id = dname + example[\"query_type\"] + str(example[\"query_id\"])\n",
    "\n",
    "    if 1 in example[\"passages\"][\"is_selected\"]:\n",
    "        prediction =  example['answers'][0]\n",
    "        context_id = example[\"passages\"][\"is_selected\"].index(1)\n",
    "        context = example[\"passages\"][\"passage_text\"][context_id]\n",
    "    else:\n",
    "        prediction = \"I don't know.\"\n",
    "        context = example[\"passages\"][\"passage_text\"][0]\n",
    "\n",
    "    # examples = make_examples(query, prediction, dname, example_id=example_id, context=context, k=k, retrieval=True, task=\"qa\", domain=\"openqa\")\n",
    "    \n",
    "    # save_examples(i, start, last_time, examples, dname)\n",
    "\n",
    "itrf_dataset_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1315"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
