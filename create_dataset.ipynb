{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from json import loads\n",
    "from transformers import LlamaTokenizer\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from qdrant_client import models, QdrantClient\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create retriever corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33176581"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents in the wikipedia corpus\n",
    "num = sum(1 for line in open(\"./data/corpora/wiki/enwiki-dec2021/text-list-100-sec.jsonl\"))\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_func(example):\n",
    "    return len(example.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len_func,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"./models/llama7b\", device_map='cuda')\n",
    "embedding = HuggingFaceBgeEmbeddings(model_name=\"./models/retriever/bge-base-en-v1.5\")\n",
    "\n",
    "# Create the retriever\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "client.create_collection(collection_name=\"retriever\", vectors_config=models.VectorParams(\n",
    "        size=768,  # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE,\n",
    "    ))\n",
    "db = Qdrant(client, \n",
    "            collection_name=\"retriever\",\n",
    "            embeddings=embedding,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: Delete the collection if it already exists\n",
    "client.delete_collection(collection_name=\"retriever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding.embed_query(\"hello iouerhfu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.as_retriever().invoke(\"Trichocladus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'title': 'Trichocladus crinitus', 'section': '', 'text': ' Trichocladus crinitus is a species of the genus Trichocladus, in the family Hamamelidaceae. It is also called black witch-hazel.'}\n"
     ]
    }
   ],
   "source": [
    "# Get test sample\n",
    "with open(\"./data/corpora/wiki/enwiki-dec2021/text-list-100-sec.jsonl\", mode=\"r\") as f:\n",
    "    for line in f:\n",
    "        doc = loads(line)\n",
    "        print(doc)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 documents, 1000 chunks, 0:00:03.381536 elapsed\n",
      "Loaded 2000 documents, 2000 chunks, 0:00:06.598974 elapsed\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m new_docs \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39mcreate_documents(documents, metadatas)         \n\u001b[1;32m     19\u001b[0m chunks_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(new_docs)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m db\u001b[38;5;241m.\u001b[39maadd_documents(new_docs)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# results = await db.aadd_documents(new_docs)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(results)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/schema/vectorstore.py:137\u001b[0m, in \u001b[0;36mVectorStore.aadd_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    136\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maadd_texts(texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/vectorstores/qdrant.py:55\u001b[0m, in \u001b[0;36msync_call_fallback.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# If the async method is not implemented, call the synchronous method\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;66;03m# by removing the first letter from the method name. For example,\u001b[39;00m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m# if the async method is called ``aaad_texts``, the synchronous method\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m# will be called ``aad_texts``.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m         sync_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m[\u001b[38;5;241m1\u001b[39m:]), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     63\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/vectorstores/qdrant.py:216\u001b[0m, in \u001b[0;36mQdrant.aadd_texts\u001b[0;34m(self, texts, metadatas, ids, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqdrant_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RestToGrpc\n\u001b[1;32m    215\u001b[0m added_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m batch_ids, points \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate_rest_batches(\n\u001b[1;32m    217\u001b[0m     texts, metadatas, ids, batch_size\n\u001b[1;32m    218\u001b[0m ):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39masync_grpc_points\u001b[38;5;241m.\u001b[39mUpsert(\n\u001b[1;32m    220\u001b[0m         grpc\u001b[38;5;241m.\u001b[39mUpsertPoints(\n\u001b[1;32m    221\u001b[0m             collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection_name,\n\u001b[1;32m    222\u001b[0m             points\u001b[38;5;241m=\u001b[39m[RestToGrpc\u001b[38;5;241m.\u001b[39mconvert_point_struct(point) \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m points],\n\u001b[1;32m    223\u001b[0m         )\n\u001b[1;32m    224\u001b[0m     )\n\u001b[1;32m    225\u001b[0m     added_ids\u001b[38;5;241m.\u001b[39mextend(batch_ids)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/vectorstores/qdrant.py:2115\u001b[0m, in \u001b[0;36mQdrant._agenerate_rest_batches\u001b[0;34m(self, texts, metadatas, ids, batch_size)\u001b[0m\n\u001b[1;32m   2112\u001b[0m batch_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(islice(ids_iterator, batch_size))\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;66;03m# Generate the embeddings for all the texts in a batch\u001b[39;00m\n\u001b[0;32m-> 2115\u001b[0m batch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aembed_texts(batch_texts)\n\u001b[1;32m   2117\u001b[0m points \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2118\u001b[0m     rest\u001b[38;5;241m.\u001b[39mPointStruct(\n\u001b[1;32m   2119\u001b[0m         \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mpoint_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2134\u001b[0m     )\n\u001b[1;32m   2135\u001b[0m ]\n\u001b[1;32m   2137\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m batch_ids, points\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/vectorstores/qdrant.py:2040\u001b[0m, in \u001b[0;36mQdrant._aembed_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m   2029\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed search texts.\u001b[39;00m\n\u001b[1;32m   2030\u001b[0m \n\u001b[1;32m   2031\u001b[0m \u001b[38;5;124;03mUsed to provide backward compatibility with `embedding_function` argument.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2037\u001b[0m \u001b[38;5;124;03m    List of floats representing the texts embedding.\u001b[39;00m\n\u001b[1;32m   2038\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2040\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39maembed_documents(\u001b[38;5;28mlist\u001b[39m(texts))\n\u001b[1;32m   2041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(embeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2042\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/schema/embeddings.py:19\u001b[0m, in \u001b[0;36mEmbeddings.aembed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maembed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Asynchronous Embed search docs.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_documents, texts\n\u001b[1;32m     21\u001b[0m     )\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load wikipedia corpus (in steps of 1000 documents)\n",
    "documents_count = 0\n",
    "chunks_count = 0\n",
    "start = time.time()\n",
    "with open(\"./data/corpora/wiki/enwiki-dec2021/text-list-100-sec.jsonl\", mode=\"r\") as f:\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    for line in f:\n",
    "        \n",
    "        json_line = loads(line)\n",
    "\n",
    "        # Split document into chunks  \n",
    "        documents.append(json_line[\"text\"])\n",
    "        metadatas.append({\"title\": json_line[\"title\"], \"section\": json_line[\"section\"]})  \n",
    "        documents_count += 1\n",
    "\n",
    "        if documents_count % 1_000 == 0:\n",
    "            new_docs = text_splitter.create_documents(documents, metadatas)         \n",
    "            chunks_count += len(new_docs)\n",
    "            await db.aadd_documents(new_docs)\n",
    "            # results = await db.aadd_documents(new_docs)\n",
    "            # print(results)\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "\n",
    "        # Save retriever every 100_000 documents\n",
    "        if documents_count % 1_000 == 0:\n",
    "             elapsed = (time.time() - start)\n",
    "             clock = str(timedelta(seconds=elapsed))\n",
    "             print(f\"Loaded {documents_count} documents, {chunks_count} chunks, {clock} elapsed\")\n",
    "\n",
    "    new_docs = text_splitter.create_documents(documents, metadatas)            \n",
    "    chunks_count += len(new_docs)\n",
    "    await db.aadd_documents(new_docs)\n",
    "    \n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "print()\n",
    "elapsed = (time.time() - start)\n",
    "clock = str(timedelta(seconds=elapsed))\n",
    "print(f\"Loaded {documents_count} documents, {chunks_count} chunks, {clock} elapsed\")\n",
    "print()\n",
    "print(\"---------------------------------------------\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cc corpus\n",
    "dataset = load_dataset('OSCAR-2201', \"unshuffled_deduplicated_en\", split='train', streaming=True)\n",
    "shuffled_dataset = dataset.shuffle(buffer_size=10_000, seed=2024)\n",
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m360_000_000\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Load next page\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Split into documents\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "while count < 360_000_000:\n",
    "    # Load next page\n",
    "    # Split into documents\n",
    "    # Pass into document retriever\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
